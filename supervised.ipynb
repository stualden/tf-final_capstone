{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad13b980",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train a variety of supervised models on the data and see how well we can do in identifying genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "monthly-director",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:57:05.860433Z",
     "start_time": "2021-05-11T10:57:05.850250Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "from   sklearn.model_selection  import train_test_split, GridSearchCV\n",
    "from   sklearn.svm              import LinearSVC\n",
    "from   sklearn                  import ensemble, tree\n",
    "from   sklearn.linear_model     import LogisticRegression\n",
    "from   sklearn.neighbors        import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82815412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:57:10.734187Z",
     "start_time": "2021-05-11T10:57:10.688007Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1200,)\n"
     ]
    }
   ],
   "source": [
    "# Read in X and y for supervised learning\n",
    "\n",
    "X = pd.read_parquet('/home/stu/final_capstone/features_final/X.parq') \n",
    "\n",
    "y = pd.read_parquet('/home/stu/final_capstone/features_final/y.parq')[\"genre\"].ravel()   # otherwise get complaints!!!\n",
    "\n",
    "#X.hvplot.table()\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sonic-interpretation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:57:13.888872Z",
     "start_time": "2021-05-11T10:57:13.878764Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)  # random_state just for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "executive-palestine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:57:32.860507Z",
     "start_time": "2021-05-11T10:57:26.691719Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Test:       0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "# First try to reproduce test results we saw in the EDA notebook\n",
    "rfb = ensemble.RandomForestClassifier(n_estimators=600,           \n",
    "                                      criterion='entropy', \n",
    "                                      max_depth=10, \n",
    "                                      min_samples_split=2, \n",
    "                                      min_samples_leaf=1, \n",
    "                                      min_weight_fraction_leaf=0.0, \n",
    "                                      max_features='auto', \n",
    "                                      max_leaf_nodes=None, \n",
    "                                      min_impurity_decrease=0.0, \n",
    "                                      min_impurity_split=None, \n",
    "                                      bootstrap=True, \n",
    "                                      oob_score=False, \n",
    "                                      n_jobs=-1,                  # Throw everything at it!!!\n",
    "                                      random_state=13,            # For reproducibility \n",
    "                                      verbose=0,\n",
    "                                      warm_start=False, \n",
    "                                      class_weight=None, \n",
    "                                      ccp_alpha=0.0, \n",
    "                                      max_samples=None\n",
    "                                     )\n",
    "rfb.fit(X_train, y_train)\n",
    "     \n",
    "print(\"R^2 Test:      \", rfb.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-husband",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:32:33.231502Z",
     "start_time": "2021-02-25T17:32:33.223772Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Now let's see if any other models can do better than the random forest; tune each of these in turn:\n",
    "\n",
    "    * KNN\n",
    "    * Linear SVC\n",
    "    * Gradient Boosting\n",
    "    * Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lucky-engagement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:58:33.407299Z",
     "start_time": "2021-05-11T10:58:32.982276Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Estimator: KNeighborsClassifier(n_jobs=-1, n_neighbors=10, weights='distance')\n",
      "  Params:       {'n_neighbors': 10, 'weights': 'distance'}\n",
      "R^2 Training:   0.7177777777777777\n",
      "R^2 Test:       0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Try a KNN classifier\n",
    "\n",
    "kn = KNeighborsClassifier(\n",
    "#                           n_neighbors=5,\n",
    "#                           weights='uniform', \n",
    "                          algorithm='auto', \n",
    "                          leaf_size=30, \n",
    "                          p=2, \n",
    "                          metric='minkowski', \n",
    "                          metric_params=None, \n",
    "                          n_jobs=-1,            # Didn't seem to speed things up much...\n",
    "                          )\n",
    "params = {\n",
    "    'n_neighbors':range(9,11), \n",
    "    'weights':['uniform','distance'],  \n",
    "}\n",
    "kng = GridSearchCV(kn, params, n_jobs=-1, cv=5, verbose=1, refit=True)\n",
    "kng.fit(X_train, y_train)\n",
    "test_score = kng.best_estimator_.score(X_test, y_test)\n",
    "print(\"Best Estimator:\", kng.best_estimator_) \n",
    "print(\"  Params:      \", kng.best_params_)\n",
    "print(\"R^2 Training:  \", kng.best_score_)           \n",
    "print(\"R^2 Test:      \", test_score)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intellectual-bangkok",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:59:11.339920Z",
     "start_time": "2021-05-11T10:59:08.714296Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[LibLinear]# of iterations 269\n",
      "Best Estimator: LinearSVC(C=0.04, max_iter=500, random_state=13, verbose=1)\n",
      "  Params:       {'C': 0.04, 'dual': True}\n",
      "R^2 Training:   0.7566666666666666\n",
      "R^2 Test:       0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "# Now try linear SVC\n",
    "\n",
    "sv = LinearSVC(penalty='l2', \n",
    "               loss='squared_hinge', \n",
    "#               dual=True, \n",
    "               tol=0.0001, \n",
    "#               C=1.0, \n",
    "               multi_class='ovr', \n",
    "               fit_intercept=True, \n",
    "               intercept_scaling=1, \n",
    "               class_weight=None, \n",
    "               verbose=1,              \n",
    "               random_state=13,        # For reproducibility\n",
    "               max_iter=500            # Default is 1000\n",
    "              ) \n",
    "params = {\n",
    "   'dual':[True, False],\n",
    "   'C':np.arange(.03,.06,.01),\n",
    "}\n",
    "svg = GridSearchCV(sv, params, n_jobs=-1, cv=5, verbose=1, refit=True)\n",
    "svg.fit(X_train, y_train)\n",
    "print('# of iterations %s' % svg.best_estimator_.n_iter_)\n",
    "print(\"Best Estimator:\", svg.best_estimator_) \n",
    "print(\"  Params:      \", svg.best_params_)\n",
    "print(\"R^2 Training:  \", svg.best_score_)           \n",
    "print(\"R^2 Test:      \", svg.best_estimator_.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floating-occasion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T11:14:24.058954Z",
     "start_time": "2021-05-11T11:01:37.468414Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.5980            1.54m\n",
      "         2           1.4449            1.52m\n",
      "         3           1.3307            1.53m\n",
      "         4           1.2339            1.52m\n",
      "         5           1.1546            1.50m\n",
      "         6           1.0807            1.48m\n",
      "         7           1.0117            1.47m\n",
      "         8           0.9552            1.46m\n",
      "         9           0.9014            1.45m\n",
      "        10           0.8543            1.44m\n",
      "        20           0.5366            1.38m\n",
      "        30           0.3594            1.30m\n",
      "        40           0.2596            1.24m\n",
      "        50           0.1946            1.21m\n",
      "        60           0.1471            1.13m\n",
      "        70           0.1143            1.04m\n",
      "        80           0.0888           58.18s\n",
      "        90           0.0702           53.39s\n",
      "       100           0.0561           48.72s\n",
      "       200           0.0066            0.00s\n",
      "Best Estimator: GradientBoostingClassifier(n_estimators=200, random_state=13, verbose=1)\n",
      "  Params:       {'max_depth': 3, 'n_estimators': 200}\n",
      "R^2 Training:   0.7666666666666666\n",
      "R^2 Test:       0.8233333333333334\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Gradient Boosting\n",
    "gb = ensemble.GradientBoostingClassifier(loss='deviance', \n",
    "                                         learning_rate=0.1, \n",
    "#                                         n_estimators=100, \n",
    "                                         subsample=1.0, \n",
    "                                         criterion='friedman_mse', \n",
    "                                         min_samples_split=2, \n",
    "                                         min_samples_leaf=1, \n",
    "                                         min_weight_fraction_leaf=0.0, \n",
    "#                                         max_depth=3, \n",
    "                                         min_impurity_decrease=0.0, \n",
    "                                         min_impurity_split=None, \n",
    "                                         init=None, \n",
    "                                         random_state=13, \n",
    "                                         max_features=None, \n",
    "                                         verbose=1, \n",
    "                                         max_leaf_nodes=None, \n",
    "                                         warm_start=False, \n",
    "                                         validation_fraction=0.1, \n",
    "                                         n_iter_no_change=None, \n",
    "                                         tol=0.0001, \n",
    "                                         ccp_alpha=0.0,\n",
    "                                        )\n",
    "params = {\n",
    "    'n_estimators':[100, 200, 300],        \n",
    "    'max_depth':[2, 3, 4],               \n",
    "}\n",
    "\n",
    "gbg = GridSearchCV(gb, params, n_jobs=-1, cv=5, verbose=1)\n",
    "gbg.fit(X_train, y_train)\n",
    "\n",
    "test_score = gbg.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Estimator:\", gbg.best_estimator_) \n",
    "print(\"  Params:      \", gbg.best_params_)\n",
    "print(\"R^2 Training:  \", gbg.best_score_)           \n",
    "print(\"R^2 Test:      \", test_score)               \n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continued-environment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T11:16:01.425351Z",
     "start_time": "2021-05-11T11:15:58.183333Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu/miniconda3/envs/latest/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.74555556        nan 0.74666667        nan 0.74222222\n",
      "        nan 0.73555556]\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of iterations 87\n",
      "Best Estimator: LogisticRegression(C=0.2, verbose=1)\n",
      "  Params:       {'C': 0.2, 'penalty': 'l2'}\n",
      "R^2 Training:   0.7466666666666667\n",
      "R^2 Test:       0.7833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Try logistic Regression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "#                       penalty='l2', \n",
    "                        dual=False, \n",
    "                        tol=0.0001, \n",
    "#                        C=1.0, \n",
    "                        fit_intercept=True, \n",
    "                        intercept_scaling=1, \n",
    "                        class_weight=None, \n",
    "                        random_state=None, \n",
    "                        solver='lbfgs', \n",
    "                        max_iter=100,\n",
    "                        multi_class='auto', \n",
    "                        verbose=1, \n",
    "                        warm_start=False, \n",
    "                        n_jobs=None, \n",
    "                        l1_ratio=None\n",
    "                       )\n",
    "params = {\n",
    "    'penalty':[None, 'l2'],\n",
    "    'C':np.arange(.1,.4, .1),\n",
    "}\n",
    "lrg = GridSearchCV(lr, params, n_jobs=-1, cv=5, verbose=1, refit=True)\n",
    "lrg.fit(X_train, y_train)\n",
    "\n",
    "test_score = lrg.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "print('# of iterations %s' % lrg.best_estimator_.n_iter_[0])\n",
    "print(\"Best Estimator:\", lrg.best_estimator_) \n",
    "print(\"  Params:      \", lrg.best_params_)\n",
    "print(\"R^2 Training:  \", lrg.best_score_)      \n",
    "print(\"R^2 Test:      \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "literary-washington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T11:17:05.973794Z",
     "start_time": "2021-05-11T11:17:03.892294Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Estimator: DecisionTreeClassifier(criterion='entropy', max_depth=9, random_state=13)\n",
      "  Params:       {'max_depth': 9, 'max_features': None}\n",
      "R^2 Training:   0.5222222222222223\n",
      "R^2 Test:       0.5266666666666666\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try a decision tree again, although I doubt it will do well:\n",
    "dt = tree.DecisionTreeClassifier(\n",
    "                            criterion='entropy', \n",
    "                            splitter='best', \n",
    "#                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "#                            max_features=None, \n",
    "                            random_state=13,                  # for reproducibility \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, \n",
    "                            min_impurity_split=None, \n",
    "                            class_weight=None, \n",
    "                            ccp_alpha=0.0\n",
    "                           )\n",
    "params = {\n",
    "   'max_depth':range(8,11),\n",
    "   'max_features':[None,'auto'],\n",
    "}\n",
    "dtg = GridSearchCV(dt, params, n_jobs=-1, cv=5, verbose=1, refit=True)\n",
    "dtg.fit(X_train, y_train)\n",
    "print(\"Best Estimator:\", dtg.best_estimator_) \n",
    "print(\"  Params:      \", dtg.best_params_)\n",
    "print(\"R^2 Training:  \", dtg.best_score_)           \n",
    "print(\"R^2 Test:      \", dtg.best_estimator_.score(X_test, y_test))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446794b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary of Supervised Learning Results\n",
    "\n",
    "\n",
    "| Model               | Key Parameters                 |  R-squared (test) | \n",
    "| :-------            | :------------:                 |      :-----:      |\n",
    "| Gradient Boosting   | 200 estimators, max depth 3    |      82.3%        |\n",
    "| Random Forest       | 600 estimators, max depth 10   |      81.3%        |\n",
    "| Linear SVC          | 270 iterations, L2             |      78.7%        |\n",
    "| Logistic Regression | 87 iterations, L2              |      78.3%        |\n",
    "| Decision Tree       | Entropy, max depth 9           |      52.7%        |\n",
    "\n",
    "\n",
    "Observations:\n",
    "\n",
    "* Random Forest and Gradient Boosting give the best performance; single decision tree is unacceptable.\n",
    "* Gradient Boosting performs the best, but at a fairly severe performance cost\n",
    "    * Slow to run even on this data set; might not scale well\n",
    "* I would choose the random forest as the best supervised learning method for the genre identification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b9142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
